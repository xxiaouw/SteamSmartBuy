{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load API key from file\n",
    "with open(\"IsThereAnyDeal_API_KEY\", \"r\") as f:\n",
    "    API_KEY = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of top 5000 steam best best selling priced games in the US from webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web-scrapping code for gathering ~ top 5000 steam best selling priced games in the US.\n",
    "\n",
    "def fetch_batch(start, count):\n",
    "    params = {\n",
    "        \"query\": \"\",\n",
    "        \"start\": start,\n",
    "        \"count\": count,\n",
    "        \"category1\": \"998\",\n",
    "        \"supportedlang\": \"english\",\n",
    "        \"hidef2p\": \"1\",\n",
    "        \"filter\": \"topsellers\",\n",
    "        \"ndl\": \"1\" \n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(\"https://store.steampowered.com/search/results/\", params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_batch(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    container = soup.find(\"div\", id=\"search_resultsRows\")\n",
    "    results = []\n",
    "\n",
    "    if container:\n",
    "        for a in container.find_all(\"a\", href=True):\n",
    "            if a.get(\"data-ds-packageid\"):  # skip packages\n",
    "                continue\n",
    "            if a.get(\"data-ds-subid\"):  # skip subs\n",
    "                continue\n",
    "\n",
    "            appid = a.get(\"data-ds-appid\")\n",
    "            href = a[\"href\"]\n",
    "            title_span = a.find(\"span\", class_=\"title\")\n",
    "            title = title_span.text.strip() if title_span else \"Unknown\"\n",
    "\n",
    "            review_span = a.find(\"span\", class_ = \"search_review_summary\")\n",
    "            if review_span:\n",
    "                tooltip = review_span.get(\"data-tooltip-html\")\n",
    "                # Use regex to extract the number from the tooltip text\n",
    "                match = re.search(r'([\\d,]+) user reviews', tooltip)\n",
    "                review_number = int(match.group(1).replace(',', '')) if match else None\n",
    "            else:\n",
    "                review_number = 0\n",
    "\n",
    "            if appid:\n",
    "                if review_number > 3000:\n",
    "                    results.append((appid, href, title))\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_all_appids(limit=5000, batch_size=50, delay=0.02):\n",
    "    all_results = []\n",
    "    for start in range(0, limit, batch_size):\n",
    "        print(f\"Fetching results {start}â€“{start+batch_size}...\")\n",
    "        html = fetch_batch(start, batch_size)\n",
    "        batch = parse_batch(html)\n",
    "        if not batch:\n",
    "            print(\"No more results.\")\n",
    "            break\n",
    "        all_results.extend(batch)\n",
    "        time.sleep(delay)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "results = get_all_appids(limit=5000)\n",
    "for appid, href, title in results:\n",
    "    print(f\"{title} (AppID: {appid}) - {href}\")\n",
    "\n",
    "print(f\"\\nTotal results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump scrapped results into CSV\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"steam_id\", \"link\", \"game\"])\n",
    "df.to_csv(\"steam_top_games.csv\", index=False)\n",
    "print(\"\\nCSV saved as steam_top_games.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get game discount history in past 10 years using IsThereAnyDeal API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out ITAD ID for games scrapped using IsThereAnyDeal API, prepping for gathering pricing history\n",
    "\n",
    "# Step 1: Load your CSV\n",
    "df = pd.read_csv(\"steam_top_games.csv\")\n",
    "\n",
    "# Step 2: Convert AppIDs to str (API requires string list)\n",
    "appids = df[\"steam_id\"].astype(str).apply(lambda x: f\"app/{x}\").tolist()\n",
    "\n",
    "# Step 3: Send request to ITAD API\n",
    "\n",
    "url = \"https://api.isthereanydeal.com/lookup/id/shop/61/v1\"\n",
    "\n",
    "response = requests.post(url, json=appids)\n",
    "mapping = response.json()\n",
    "\n",
    "# Step 4: Attach the ITAD game ID to the dataframe\n",
    "# Clean appid (remove \"app/\") to match with df[\"steam_id\"]\n",
    "df[\"itad_id\"] = df[\"steam_id\"].astype(str).apply(lambda x: mapping.get(f\"app/{x}\"))\n",
    "\n",
    "# Step 5: Save the updated dataframe\n",
    "df.to_csv(\"steam_top_games_with_itad_ids.csv\", index=False)\n",
    "print(\"Saved updated CSV as steam_top_games_with_itad_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ITAD ID for games to gather pricing history using IsThereAnyDeal API\n",
    "\n",
    "# Load the CSV with steam_id, game name, itad_id\n",
    "df = pd.read_csv(\"steam_top_games_with_itad_ids.csv\")\n",
    "\n",
    "PRICE_HIST_URL = \"https://api.isthereanydeal.com/games/history/v2\"\n",
    "SINCE_DATE = \"2015-06-01T00:00:00Z\"\n",
    "\n",
    "# Create a new column to store raw price log JSON\n",
    "df[\"price_logs\"] = None\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Fetching price log info\"):\n",
    "    itad_id = row.get(\"itad_id\")\n",
    "    if pd.isna(itad_id):\n",
    "        continue\n",
    "\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"id\": itad_id,\n",
    "        \"shops\": 61, \n",
    "        \"since\": SINCE_DATE\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(PRICE_HIST_URL, params=params)\n",
    "        resp.raise_for_status()\n",
    "        price_data = resp.json()\n",
    "\n",
    "        # Store raw price log list directly\n",
    "        df.at[idx, \"price_logs\"] = str(price_data)  # stringify list for CSV compatibility\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {row.get('game')} ({itad_id}): {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"steam_top_games_with_price_logs.csv\", index=False)\n",
    "print(\"Saved raw price logs to steam_top_games_with_price_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial clean-ups of gathered pricing history data\n",
    "\n",
    "# Load your full price log dataset\n",
    "df = pd.read_csv(\"steam_top_games_with_price_logs.csv\")\n",
    "print(f\"Totaling {len(df)} games before clean-up.\\n\")\n",
    "\n",
    "# Parse the 'price_logs' column from string to list\n",
    "df[\"parsed_logs\"] = df[\"price_logs\"].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Identify rows with empty price log lists\n",
    "empty_log_df = df[df[\"parsed_logs\"].apply(lambda logs: len(logs) == 0)]\n",
    "\n",
    "# Print the list of missing games\n",
    "print(\"Games with NO price history on IsThereAnyDeal:\\n\")\n",
    "for _, row in empty_log_df.iterrows():\n",
    "    print(f\"- {row['game']} (Steam ID: {row['steam_id']})\")\n",
    "\n",
    "# Filter out those rows (i.e., keep only non-empty logs)\n",
    "df_cleaned = df[df[\"parsed_logs\"].apply(lambda logs: len(logs) > 0)].drop(columns=[\"parsed_logs\"])\n",
    "\n",
    "print(f\"\\nClean-up complete, {len(df_cleaned)} games left after clean-up.\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_cleaned.to_csv(\"steam_top_games_with_price_logs_cleaned.csv\", index=False)\n",
    "print(\"Saved cleaned dataset to steam_top_games_with_price_logs_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get game basic information using Steam's own API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_game_info (steam_id):\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(f\"https://store.steampowered.com/api/appdetails?appids={steam_id}&cc=us&l=en\", headers=HEADERS)\n",
    "\n",
    "    # in case missing information, return all None\n",
    "    # try:\n",
    "    info = r.json().get(f'{steam_id}', {}).get(\"data\", {})\n",
    "\n",
    "    # get game related information\n",
    "\n",
    "    # store as a list if it's a list, or wrap in list if it's a single string\n",
    "    publisher = info.get(\"publishers\", []) if isinstance(info.get(\"publishers\"), list) else [info.get(\"publishers\")] if info.get(\"publishers\") else []\n",
    "    developer = info.get(\"developers\", []) if isinstance(info.get(\"developers\"), list) else [info.get(\"developers\")] if info.get(\"developers\") else []\n",
    "                                                                                                      \n",
    "    category  = str(info.get(\"categories\")) if \"categories\" in info else None\n",
    "    genre     = str(info.get(\"genres\")) if \"genres\" in info else None\n",
    "\n",
    "    header_image = str(info.get(\"header_image\")) if \"categories\" in info else None\n",
    "\n",
    "    # release_date = str(datetime.strptime(info.get(\"release_date\", {}).get('date'), \"%b %d, %Y\")) if  # for partial storing in json\n",
    "    date_str = info.get(\"release_date\", {}).get('date')\n",
    "    if isinstance(date_str, str) and date_str.strip():\n",
    "        release_date = str(datetime.strptime(date_str, \"%b %d, %Y\"))\n",
    "    else:\n",
    "        release_date = None\n",
    "    \n",
    "    # rec = int(info.get(\"recommendations\").get(\"total\")) if \"recommendations\" in info else 0\n",
    "    score = info.get(\"metacritic\", {}).get(\"score\")\n",
    "    \n",
    "    # platforms_avaliable = info.get(\"platforms\", {}) if isinstance(info.get(\"platforms\"), dict) else {info.get(\"platforms\")} if info.get(\"platforms\") else {}\n",
    "    platforms = info.get(\"platforms\", {})\n",
    "    is_windows = int(platforms.get(\"windows\", False))\n",
    "    is_mac = int(platforms.get(\"mac\", False))\n",
    "    is_linux = int(platforms.get(\"linux\", False))\n",
    "    # platforms_avaliable = sum(info.get(\"platforms\").values()) if \"platforms\" in info else None\n",
    "    \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error occured at index {idx} for {steam_id}: {e}\")\n",
    "    #     # publisher,developer,category,genre,release_date,score, platforms_avaliable = None, None, None, None, None, None, None\n",
    "\n",
    "    r_rec = requests.get(f\"https://store.steampowered.com/appreviews/{steam_id}?json=1&language=all\", headers=HEADERS)\n",
    "    query_summary = r_rec.json().get(\"query_summary\", {})\n",
    "    rec_tot = int(query_summary.get(\"total_reviews\", {})) if \"total_reviews\" in query_summary else None\n",
    "    rec_pos = int(query_summary.get(\"total_positive\", {})) if \"total_positive\" in query_summary else None\n",
    "    rec_desc = str(query_summary.get(\"review_score_desc\", {})) if \"review_score_desc\" in query_summary else None\n",
    "    \n",
    "    time.sleep(1.5)\n",
    "    return (publisher,developer,category,genre,header_image,release_date,score,rec_pos,rec_tot,rec_desc,is_windows,is_mac,is_linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"steam_top_games_with_price_logs_cleaned.csv\")\n",
    "\n",
    "# collect and add basic game information to df\n",
    "df_new = df.copy()\n",
    "game_info = []\n",
    "\n",
    "try:\n",
    "    for idx, row in tqdm(df_new.iterrows(), total=len(df_new), desc=\"Fetching game info\"):\n",
    "        current_game_info = get_game_info(row[\"steam_id\"])\n",
    "        game_info.append(current_game_info)\n",
    "\n",
    "        if idx % 100 == 0 and idx != 0:\n",
    "            with open(\"game_info.pkl\", \"wb\") as f:\n",
    "                pickle.dump(game_info, f)\n",
    "            # print(f\"Auto-saved at index {idx}\")\n",
    "            # print(f\"Testing integrity of data: the last game is from {current_game_info[0]} and is rated as {current_game_info[8]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occured at index {idx} for {row[\"steam_id\"]}: {e}\")\n",
    "    with open(\"game_info.pkl\", \"wb\") as f:\n",
    "        pickle.dump(game_info, f)\n",
    "    print(\"Partial data saved to 'game_info.pkl'\")\n",
    "    raise  # re-raise the exception to stop the script\n",
    "\n",
    "with open(\"game_info.pkl\", \"wb\") as f:\n",
    "                pickle.dump(game_info, f)\n",
    "print(\"Data saved to 'game_info.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"game_info.pkl\", \"rb\") as f:\n",
    "    game_info = pickle.load(f)\n",
    "\n",
    "df = pd.read_csv(\"steam_top_games_with_price_logs_cleaned.csv\")\n",
    "df_new = df.copy()\n",
    "df_new[[\"publishers\", \"developers\", \"categories\", \"genres\", \"header_image\", \"release_dates\", \\\n",
    "        \"metacritic_scores\", \"positive_review\", \"total_review\", \"review_desc\", \\\n",
    "        \"is_windows\", \"is_mac\", \"is_linux\"]] = pd.DataFrame(game_info, index=df_new.index)\n",
    "\n",
    "# Save dataset\n",
    "df_new.to_csv(\"steam_top_games_with_price_logs_and_game_info.csv\", index=False)\n",
    "print(\"Saved cleaned dataset to steam_top_games_with_price_logs_and_game_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate and errorsome games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"steam_top_games_with_price_logs_and_game_info.csv\")\n",
    "print(df.info())\n",
    "print(f\"\\nunique steam_id {df.steam_id.nunique()}\\n\")\n",
    "print(f\"\\nunique publishers {df.publishers.nunique()}\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate steam_id:\n",
    "\n",
    "# Group by 'steam_id' and count occurrences\n",
    "duplicates = df.groupby('steam_id').size().reset_index(name='count')\n",
    "duplicates = duplicates[duplicates['count'] > 1]\n",
    "\n",
    "# Filter the original df for these steam_ids\n",
    "non_unique_df = df[df['steam_id'].isin(duplicates['steam_id'])]\n",
    "\n",
    "# Show the 'steam_id' and 'game' columns\n",
    "print(non_unique_df[['steam_id', 'game']].sort_values(by='steam_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing: \n",
    "# Removing steam_id 22330 and 40960 completely.\n",
    "# Removing It Takes Two Friend's Pass\n",
    "# For the remaining repeating rows, keep one of them.\n",
    "\n",
    "df = pd.read_csv(\"steam_top_games_with_price_logs_and_game_info.csv\")\n",
    "\n",
    "# 1 Remove rows with steam_id 22330 and 40960\n",
    "df = df[~df['steam_id'].isin([22330, 40960,1794960])]\n",
    "\n",
    "# 2 Remove rows where game is \"It Takes Two Friend's Pass\"\n",
    "df = df[df['game'] != \"It Takes Two Friend's Pass\"]\n",
    "\n",
    "# 3 For remaining duplicate steam_id rows, keep the first occurrence\n",
    "df = df.drop_duplicates(subset='steam_id', keep='first')\n",
    "\n",
    "df.to_csv(\"steam_top_games_with_price_logs_and_game_info.csv\", index=False)\n",
    "print(f\"Saved cleaned dataset to steam_top_games_with_price_logs_and_game_info.csv, length = {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using RAWG API to supplement meta-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"steam_top_games_with_price_logs_and_game_info.csv\")\n",
    "\n",
    "with open(\"RAWG_API_Key\", \"r\") as f:\n",
    "    RAWG_API_Key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "records = []\n",
    "failed_titles = []\n",
    "\n",
    "for title in tqdm(df['game'],desc=\"Fetching meta score info\"):\n",
    "    try:\n",
    "        # Step 1: Search for the game on RAWG (platform 4 = PC)\n",
    "        search_url = 'https://api.rawg.io/api/games'\n",
    "        search_params = {\n",
    "            'search': title,\n",
    "            'key': RAWG_API_Key,\n",
    "            'page_size': 1,\n",
    "            'platforms': 4\n",
    "        }\n",
    "        search_resp = requests.get(search_url, params=search_params).json()\n",
    "\n",
    "        if not search_resp['results']:\n",
    "            continue\n",
    "\n",
    "        game_id = search_resp['results'][0]['id']\n",
    "        game_name = search_resp['results'][0]['name']\n",
    "\n",
    "        # Step 2: Fetch full game details using the ID\n",
    "        game_resp = requests.get(\n",
    "            f'https://api.rawg.io/api/games/{game_id}',\n",
    "            params={'key': RAWG_API_Key}\n",
    "        ).json()\n",
    "\n",
    "        # Extract PC-specific Metacritic score if available\n",
    "        pc_score = None\n",
    "        for p in game_resp.get('metacritic_platforms', []):\n",
    "            if p['platform']['name'].lower() == 'pc':\n",
    "                pc_score = p['metascore']\n",
    "                break\n",
    "\n",
    "        records.append({\n",
    "            'search_name': title,\n",
    "            'resolved_name': game_name,\n",
    "            'overall_metacritic_score': game_resp.get('metacritic'),\n",
    "            'pc_metacritic_score': pc_score\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {title}: {e}\")\n",
    "        failed_titles.append(title)\n",
    "\n",
    "# Convert to DataFrame\n",
    "RAWG_meta_df = pd.DataFrame(records)\n",
    "RAWG_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for title in tqdm(failed_titles,desc=\"Retrying fetching meta score info for failed cases\"):\n",
    "    try:\n",
    "        # Step 1: Search for the game on RAWG (platform 4 = PC)\n",
    "        search_url = 'https://api.rawg.io/api/games'\n",
    "        search_params = {\n",
    "            'search': title,\n",
    "            'key': RAWG_API_Key,\n",
    "            'page_size': 1,\n",
    "            'platforms': 4\n",
    "        }\n",
    "        search_resp = requests.get(search_url, params=search_params).json()\n",
    "\n",
    "        if not search_resp['results']:\n",
    "            continue\n",
    "\n",
    "        game_id = search_resp['results'][0]['id']\n",
    "        game_name = search_resp['results'][0]['name']\n",
    "\n",
    "        # Step 2: Fetch full game details using the ID\n",
    "        game_resp = requests.get(\n",
    "            f'https://api.rawg.io/api/games/{game_id}',\n",
    "            params={'key': RAWG_API_Key}\n",
    "        ).json()\n",
    "\n",
    "        # Extract PC-specific Metacritic score if available\n",
    "        pc_score = None\n",
    "        for p in game_resp.get('metacritic_platforms', []):\n",
    "            if p['platform']['name'].lower() == 'pc':\n",
    "                pc_score = p['metascore']\n",
    "                break\n",
    "\n",
    "        records.append({\n",
    "            'search_name': title,\n",
    "            'resolved_name': game_name,\n",
    "            'overall_metacritic_score': game_resp.get('metacritic'),\n",
    "            'pc_metacritic_score': pc_score\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {title}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAWG_meta_df = pd.DataFrame(records)\n",
    "RAWG_meta_df.to_csv('RAWG_metacritic_scores.csv', index=False)\n",
    "RAWG_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge based on 'game' matching 'search_name'\n",
    "df = pd.read_csv(\"steam_top_games_with_price_logs_and_game_info.csv\")\n",
    "RAWG_meta_df = pd.read_csv(\"RAWG_metacritic_scores.csv\")\n",
    "merged_df = RAWG_meta_df.merge(\n",
    "    df[['game', 'metacritic_scores']].rename(columns={'metacritic_scores': 'steam_metacritic_score'}),\n",
    "    left_on='search_name',\n",
    "    right_on='game',\n",
    "    how='left'\n",
    ").drop(columns='game')  # Drop redundant 'game' column after merge\n",
    "merged_df\n",
    "\n",
    "merged_df['metacritic_score'] = (\n",
    "    merged_df['pc_metacritic_score']\n",
    "    .combine_first(merged_df['steam_metacritic_score'])\n",
    "    .combine_first(merged_df['overall_metacritic_score'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['metacritic_score'] = merged_df['metacritic_score'].fillna(merged_df['metacritic_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    merged_df[['search_name', 'metacritic_score']],\n",
    "    how='left',\n",
    "    left_on='game',\n",
    "    right_on='search_name'\n",
    ").rename(columns={'metacritic_score': 'metacritic_scores_augmented'}).drop(columns=['search_name'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('steam_top_games_with_price_logs_and_game_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final filtering to remove games with less than 5000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_filter = pd.read_csv('steam_top_games_with_price_logs_and_game_info.csv')\n",
    "final_filter = final_filter[final_filter['total_review'] > 5000]\n",
    "final_filter.to_csv('steam_top_games_with_price_logs_and_game_info.csv', index=False)\n",
    "final_filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
